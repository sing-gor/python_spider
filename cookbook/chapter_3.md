## 爬取淘宝商品列表数据  
这里直接上代码，详情看代码的注释
```python
import requests
import re
import json
import pandas
# 打开网页函数
def get_response(url):
    headers = {
        'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"}
    response = requests.get(url, headers) # 加上浏览器头，以防被禁
    response.encoding = 'utf-8'      # 指定编码格式
    #response.encoding = 'gbk'      # 指定编码格式
    return response

def main():
    # 搜索的商品名
    commodity = '火鸡面'
    # 拼接url 生成商品搜索页
    base_url = f'https://s.taobao.com/search?q={ commodity }'
    # 打开该url获取内容
    response = get_response(base_url)
    # 由于 淘宝的数据是在 网页源代码中的第8个script语句中的第一段json数据中
    # 正常的话无法通过bs4、xpath等方式解析。
    # 只能通过正则来获取数据
    req = 'g_page_config = (.*?)g_srp_loadCss'
    # 由于re.findall方法获取到的是一个列表，所以需要取第一个元素，并把字符中的空格去掉
    items_list = re.findall(req,response.text,re.S)[0].strip()
    # items_list是一个字符串，由于结尾有个;  不符合json的格式要求，去掉它
    # 使用json中的load方法，转换成pyhton可操作的字典类型
    js = json.loads(items_list[:-1])
    jd = js['mods']['itemlist']['data']['auctions'] #.keys())
    # 使用pandas的数据呈现方法转化之前处理过的数据
    df = pandas.DataFrame(jd)
    # 在下面逐一输入键名进行观察需要的数据。用浏览器打开当前文件,进行筛选
    # 筛选完成后生成一个txt文件
    df[['category','raw_title','view_price','item_loc','view_sales']].to_html(f'{commodity}.html')





# 你这个问题结合这个文件来说吧
# 当你运行这个文件时，只会执行在if __name__ == "__main__":下面的main（）函数
# 当这个文件被作为模块调用时，则不会执行下面的main（）函数


if __name__ == "__main__":
    main()
```  
这里直接上代码的原因是，上面代码获取到的数据是不完整，被淘宝的防爬措施处理过的数据。  
大家可以看一下，运行上面代码生成的文件，用浏览器打开，就可以看到。很多数据是我们不需要的。  
无啦啦来点，羽毛、衣服、叉子、拼图之类的商品信息。  
影响我们后期的数据分析。  
由此，接下来讲一下网站的反爬措施。  
## 反爬措施  
### 浏览器头  
这是最最基本的反爬措施。  
就是我们之前爬取小说时提过的**headers**中的**User-Agent**参数，这是最基本的。  
如果没有写这个参数，我们去请求对方的服务器时，我们的额头上写着**我是爬虫，来封我啊**。  
关于User-Agent，谷歌一下，一大堆。复制粘贴即可。  
### 访问速度  
访问速度，之前我们写的爬取小说的小爬虫，就没有限制速度这个措施。  
正常来说，爬取大型网站，它会限制我们的每小时访问的次数，通常来讲，把速度设为我们平常访问的速度即可。  
当我们的访问速度触碰到网站的红线的话，一般会让你输入验证码，来确认你是不是机器人？  
有些网站会直接限制你访问。要等一定时间后才让你访问。  
注意，尽量不要去破解**由于访问速度过快导致让你输入验证码的图片**，即使破解成功，网站通常会把你列入一个名单（小黑屋和正常用户中间），当你遇到下一次需要输入验证码的时候。即使输入正确，不出3个页面。小黑屋见（以亚马逊为例）。  
### ip地址  
简单来说，你的ip地址，是唯一的，你一个人的访问速度过快，网站的服务器就会按照你的ip地址封你ip。禁止你访问。  
### 请求的参数  
上面我们说过浏览器的头，其实还有好几个参数，例如Accept、Accept-Encoding、Connection、Cookie、Referer等等。  
有些网站需要一些特殊的参数，例如知乎。  
不过知乎需要的参数经常变化，就不举例了。  
### 登录后方可浏览内容  
例如，微博、知乎之类的。  
需要我们用户登录后方可访问网页的内容。  
简单来说需要登录后的**Cookie**   
### js和js加密  
有时候，我们查看一个网页的源代码，发现里面空空如也。只有一堆js代码，甚至没有（vue-cli做的网站）。而想查看js代码，发现只有一大堆乱码。  
## 举几个例子  
## 淘宝的反爬措施  
让你爬，但爬不到正确的数据。  
淘宝的用户太多，淘宝怕**误杀**。  
它服务器只检测你是否带有浏览器头（像上面一样）。  
但如果你仅仅只有浏览器头，它只会给一丢丢有用的数据。而大部分的数据都是错误的数据。  
一旦你请求过快，就需要输入验证码了。  
所以构造好请求头，访问几乎没有什么问题。  
这个留到后面用scrapy淘宝爬取再讲。  
## 和图书的反爬措施  
这个网站，是我在鱼C论坛上遇到过的一个问题。  
挺让人火大的，大家可以用之前爬取小说的代码去试试。  
改一下解析式就可以用了。  
很好爬，但是，你爬取下来的只是一堆**烂码**  
不信，你随便打开一篇小说正文的网页源代码看看？？  
这能看吗？？  
它用了js加载网页源代码中的烂码后，再呈现到网页上让你看到。  
现在知道它是js加载的，然后去查看他的js代码，你会发现js中只有一大堆乱码。  
不过呢，我已经破解了它正文的加密方法了。  
代码就不放出来，只讲思路。  
首先，找一下它正文部分中的div的元素选择器是那一个。  
然后到js中寻找应用了该选择器的函数。  
看一下是怎么处理的。  
提示一下，密文部分是用编码混淆的。  
然后根据解密的密文通过字典的形式重新排列就可以获得真正的内容了。  
  
