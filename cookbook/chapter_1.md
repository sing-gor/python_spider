## 第一条小爬虫  
写任何代码前，我们必须确定这次写的代码是用来干什么的？  
这次我们要把[古龙小说全集](https://www.gulongwang.com/)中的所有小说按照**书名**保存成一个txt文件。  
这次使用python中的第三方包**requests**来进行爬取。  
大家可以打开上面的网址到里面进行观察一下该网站的网址的变化（随便点击几本小说进行观察）  
思考一下，我们人类是怎么获取里面的内容的？  
不难发现，我们进入首页，看到一堆书名。  
点开书名后到达章节列表，点开章节后进入小说内容的详情页，也就是我们需要获取的小说内容。  
这里我们可以把这个网站分为三层结构，如下：
- 书名列表页（首页）
- 章节列表页
- 内容页  

## 打开首页    
关于代码的解释写在代码的注释中。  
首先，我们先把终端的操作路径切换到我们需要创建爬虫文件的目录下。  
我这里是直接在桌面上创建了一个名为python_spier的目录。  
进入该目录并切换到虚拟环境。  
新开一个终端：  
`cd Desktop/python_spider  && source activate py_spider`  
新建一个名为requests_0.py的文件  
`touch requests_0.py`  
写入以下内容：
```python
import requests   # 导入requests包

# 把我们要爬取的网址赋值给url这个变量
url = 'https://www.gulongwang.com/'

# 使用requests中的get方法来请求我们的网址
# 采取该方法后会返回一个响应,相当与我们打开网页后会返回页面一样

# 简单来说，我们在浏览器输入一个网址是请求该网站
# 返回会的内容是该网站对我们的请求的响应。
# 响应中包括cookice、网页内容、状态码、header等

response = requests.get(url=url)

# 直接打印response是打印出该请求的状态码
print(response)

# 打印出该响应的文本信息，也就是该网址的网页源代码
print(response.text)
```
上面的代码就是最最基本打开一个网址的代码。  
回头想一下，我们打开首页后要做什么呢？？  
要**点击书名**，然后进入章节列表。而我们输入对应书名的链接也是可以进入该章节列表的。  
例如在浏览器中输入`https://www.gulongwang.com/cang/`也是可以进入该小说的章节列表。  
和在首页点击的效果是一样的。  
所以，我们可以在首页的内容中获取小说的章节列表的url。  
怎么获取呢？  
这个时候需要用到解析方法。  
通常我们用的解析网页元素的方法有bs4、xpath、re、pyquery、css选择器、pandas（对表结构的解析很强后面会介绍）等等。  
现在我们采取bs4的方式进行解析，这里就不介绍bs4的用法了。可自行查阅[bs4的文档](https://beautiful-soup-4.readthedocs.io/en/latest/)进行学习，挺简单的。  
同时也推荐一款在谷歌浏览器的插件**infolife**可以帮助我们初学者快速获取**解析式**，文件在该项目对应的github上，[地址]()
